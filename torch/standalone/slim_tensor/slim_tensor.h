#pragma once

#include <cassert>
#include <cstdint>
#include <cstring>
#include <stdexcept>
#include <utility>

#include <torch/csrc/inductor/aoti_standalone/utils.h>
#include <torch/standalone/slim_tensor/storage.h>
#include <torch/standalone/slim_tensor/utils.h>

namespace torch::standalone {

class SlimTensor {
 public:
  SlimTensor(
      Storage&& storage,
      const ArrayRef& sizes,
      const ArrayRef& strides,
      c10::ScalarType dtype,
      int64_t storage_offset = 0)
      : storage_(std::move(storage)),
        sizes_(sizes),
        strides_(strides),
        dtype_(dtype),
        storage_offset_(storage_offset),
        numel_(compute_numel(sizes_)) {}

  SlimTensor() = delete;
  SlimTensor(const SlimTensor&) = default;
  SlimTensor& operator=(const SlimTensor&) = default;
  SlimTensor(SlimTensor&&) = default;
  SlimTensor& operator=(SlimTensor&&) = default;

  ~SlimTensor() = default;

  void reset() {
    // Decrement the refcount of the storage
    storage_.reset();
  }

  // Accessors
  Storage storage() const {
    return storage_;
  }

  ArrayRef sizes() const {
    return sizes_;
  }

  int64_t size(size_t dim) const {
    return sizes_[dim];
  }

  ArrayRef strides() const {
    return strides_;
  }

  int64_t stride(size_t dim) const {
    return strides_[dim];
  }

  c10::ScalarType dtype() const {
    return dtype_;
  }

  const c10::Device& device() const {
    return storage_->device();
  }

  c10::DeviceType device_type() const {
    return storage_->device_type();
  }

  c10::DeviceIndex device_index() const {
    return storage_->device_index();
  }

  int64_t storage_offset() const {
    return storage_offset_;
  }

  size_t numel() const {
    return numel_;
  }

  size_t nbytes() const {
    return compute_nbytes(numel_, dtype_);
  }

  size_t dim() const {
    return sizes_.size();
  }

  void* data_ptr() const {
    return storage_->data();
  }

  bool is_contiguous() const {
    // bogus, always return true for now
    return true;
  }

  SlimTensor as_strided_(
      ArrayRef sizes,
      ArrayRef strides,
      int64_t storage_offset) {
    sizes_ = std::move(sizes);
    strides_ = std::move(strides);
    storage_offset_ = storage_offset;
    return *this;
  }

  SlimTensor copy_(const SlimTensor& other) {
    storage_->clone(other.storage(), other.nbytes(), other.storage_offset());
    return *this;
  }

  SlimTensor to(const c10::Device& device) const {
    // Does not mutate the current tensor, but returns a new tensor
    if (device == storage_->device()) {
      return *this;
    }
    Storage new_storage(new MaybeOwningStorage(nbytes(), device));
    new_storage->clone(storage_, nbytes(), storage_offset_);
    return SlimTensor(
        std::move(new_storage), sizes_, strides_, dtype_, storage_offset_);
  }

  SlimTensor to(c10::ScalarType dtype) const {
    throw std::runtime_error("TBD: to(dtype)");
  }

  SlimTensor permute(const int64_t* dims, int64_t dims_len) const {
    const int64_t ndim = static_cast<int64_t>(this->dim());

    TORCH_CHECK(
        ndim == dims_len, "permute: dims length must be equal to tensor.dim()")

    const auto old_sizes = this->sizes();
    const auto old_strides = this->strides();

    int64_t* new_sizes = new int64_t[dims_len];
    int64_t* new_strides = new int64_t[dims_len];
    std::vector<bool> seen_dims(dims_len, false);

    for (int64_t i = 0; i < dims_len; i++) {
      int64_t d = maybe_wrap_dim(dims[i], ndim);
      TORCH_CHECK(!seen_dims[d], "permute: duplicate dims are not allowed");
      seen_dims[d] = true;
      new_sizes[i] = old_sizes[d];
      new_strides[i] = old_strides[d];
    }

    ArrayRef sizes_ref{new_sizes, static_cast<size_t>(dims_len)};
    ArrayRef strides_ref{new_strides, static_cast<size_t>(dims_len)};

    SlimTensor result = *this;
    result.as_strided_(sizes_ref, strides_ref, this->storage_offset());
    return result;
  }

 private:
  // device_type_ and device_index_ are stored in storage_
  Storage storage_;
  // Sizes and strides are either generated by AOTI as static arrays,
  // or dynamically generated, e.g. when in fallback eager ops.
  ArrayRef sizes_;
  ArrayRef strides_;
  c10::ScalarType dtype_;
  int64_t storage_offset_;
  size_t numel_;
};

// The returned SlimTensor owns the underlying storage
inline SlimTensor create_empty_tensor(
    const ArrayRef& sizes,
    const ArrayRef& strides,
    c10::ScalarType dtype,
    const c10::Device& device = CPU_DEVICE,
    int64_t storage_offset = 0,
    bool own_sizes_and_strides = false) {
  ArrayRef new_sizes =
      ArrayRef(sizes.data(), sizes.size(), own_sizes_and_strides);
  ArrayRef new_strides =
      ArrayRef(strides.data(), strides.size(), own_sizes_and_strides);
  size_t nbytes = compute_nbytes(sizes, dtype);
  Storage storage(new MaybeOwningStorage(nbytes, device));
  return SlimTensor(
      std::move(storage), new_sizes, new_strides, dtype, storage_offset);
}

// The returned SlimTensor does not own the underlying storage
inline SlimTensor create_tensor_from_blob(
    void* data,
    const ArrayRef& sizes,
    const ArrayRef& strides,
    c10::ScalarType dtype,
    const c10::Device& device = CPU_DEVICE,
    int64_t storage_offset = 0) {
  if (data == nullptr) {
    throw std::runtime_error("data pointer can not be nullptr");
  }
  Storage storage(new MaybeOwningStorage(data, device));
  return SlimTensor(std::move(storage), sizes, strides, dtype, storage_offset);
}
} // namespace torch::standalone
